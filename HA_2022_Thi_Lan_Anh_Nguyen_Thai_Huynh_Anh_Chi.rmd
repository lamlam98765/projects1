---
title: "Statistical Computing: Home Assignment 2022"
output: html_notebook
---

Solve all the tasks in this R notebook template. Write sufficient comments for your code. Codes that are not written using the template and/or that return error messages will not be evaluated. If you are working in groups, make sure to note down every participant's name, stu-number and e-mail address.

*Authors:* 
Thi Lan Anh Nguyen - 1162062 - stu231566@mail.uni-kiel.de
Thai Huynh Anh Chi - 1162964 - stu232183@mail.uni-kiel.de


Submission: Submit your scripts via OLAT (Home Assignment -> Submission) until the end of July 2nd (until 2.07.2022, 23:59).


# Part I.

Consider the following function:

\[f(x,y) = -\frac{((x^2-1)(y^2-4) + x ^2 +y^2 -5)}{(x^2 + y^2 + 1)^2}\].

a) Plot the 3D surface of the function f on $[-3,3]^2$ and a contour plot.
```{r}
fxy = function(x,y){
  - ( ((x^2-1) *(y^2-1) + x^2 + y^2 -5)/(x^2 + y^2 +1)^2 )
}
x = seq(-3, 3, length = 100)
y = x
z = outer(x, y, fxy)
persp(x, y, z,theta = 30, phi = 40)
contour(x, y, z, nlevels = 20, drawlabels = F)

```

b) Write R functions for finding the gradient.

```{r}
fxy = function(x){
  - ( ((x[1]^2 -1) *(x[2]^2 -1) + x[1]^2 + x[2]^2 -5)/(x[1]^2 + x[2]^2 +1)^2 )
}

f.prime = function(x){
	f.prime.dx = 2*x[1]/(x[1]^2 + x[2]^2 +1)^3 *(x[1]^2* x[2]^2 -3*x[1]^2 -x[2]^4 +2*x[2]^2 +1)
	f.prime.dy = 2*x[2]/(x[1]^2 + x[2]^2 +1)^3 *(x[1]^2 *x[2]^2 -x[1]^4 -7*x[1]^2 -2)
	out = matrix(c(f.prime.dx,f.prime.dy),ncol=1)
	return(out)
}

```

c) Find a maximum using the steepest ascent algorithm. Use 100 iterations and perform backtracking for $\alpha$. Choose your own starting values. 

```{r}

# starting point
x = c(-2,-2)
M = diag(-1, 2, 2)
itr = 100
alpha.default = 1
alpha = alpha.default
x.values = matrix(0, itr+1, 2) #initialize a matrix, rows = itr+1, column = 2
x.values[1,] = x #1st element is the starting points

#search loop
for (i in 1:itr){
  hessian.inv = solve(M)
  xt = x - alpha*hessian.inv %*% f.prime(x)
  #Reduce alpha until a correct step is reached - backtracking
  while(fxy(xt) < fxy(x)){ 
    alpha =alpha/2
    xt = x - alpha*hessian.inv %*% f.prime(x)
  }
  x.values[i+1, ] = x = xt
  alpha = alpha.default #adjust back to = 1
}

```

d) Return the value of the maximum and the gradient. Plot the convergence path on the contour plot.

```{r}
x		       # FINAL ESTIMATE
fxy(x) 		   # OBJECTIVE FUNCTION AT ESTIMATE
f.prime(x) # GRADIENT AT ESTIMATE
#f.prime is very close to 0
```

# Part II.

The goal of this task is to select a best subset of regressors to explain a sale price for houses with the help of a linear model. The data are contained in houseprice.RData and has already been cleaned. There is a readme file for more details. In order to compare the models (changes in 1-neighborhood) use Bayesian Information Criterion (BIC).

a) Download the dataset "houseprice.RData" from OLAT and open it in R. Set up 20 iterations and 8 random starts for a random starts local search procedure.

```{r}
#open file houseprice

# last column is the dependent variable
data.sub = houseprice[, 2:61] # X
y = houseprice[, 62]
n = length(y) # number of obs
m = ncol(data.sub) #number of var
num.starts = 8
runs = matrix(0, num.starts, m)
itr = 20
runs.bic = matrix(0,num.starts,itr)

```

b) Draw 8 random starts from a Bernoulli distribution.Use set.seed(678) and p=0.5.

```{r}
#Since each run is the bunch of 1 and 0 -> can generate randomly by Benoulli dist
set.seed(678)
for(i in 1:num.starts){
  runs[i,] = rbinom(m, 1, 0.5)
} #-> 8 starts contains 60 predictors with 1 - included and 0 -not included
# at the end, we want to have convergence towards 8 best models then use bic to decide which one is the best

colnames(runs) = colnames(data.sub)
```

c) Perform a random search to find a best linear model with respect to Bayesian Information Criterion (BIC).

```{r}
for(k in 1:num.starts){
  run.current = runs[k,] # one model each time
  
  #1. Random start iteration - 1st run
  for(j in 1:itr){
    run.vars = data.sub[, run.current ==1] #only picks those included -> create subset
    g = lm(y~ ., data = run.vars) #fit new model
    run.bic = extractAIC(g, k = log(n))[2]
    run.next = run.current 
    
    # 2. testing all models in 1-neighborhood and selecting the lowest bic
    for(i in 1:m){
      run.step = run.current
      run.step[i] =! run.current[i]
      run.vars = data.sub[, run.step == 1]
      g = lm(y~., run.vars)
      run.step.bic = extractAIC(g, k= log(n))[2]
      if(run.step.bic < run.bic){ #if there's an improvement -> replace -> continue searching
        run.next = run.step
        run.bic = run.step.bic
      }
    } # after this loop, find the min in the k-th in 1-neighborhood model:
    run.current = run.next
    runs.bic[k, j] = run.bic
  }
  runs[k, ] = run.current # update row runs by its newly found model 
}

```

d) Display the 8 convergence paths graphically and determine the optimal model. Finally, fit the optimal model.

```{r}
runs 		  # selected predictors
runs.bic 	# BIC values

plot(1:(itr*num.starts), -c(t(runs.bic)), xlab = "Cummulative iteration", ylab = "Negative BIC", type = "n")


for(i in 1:num.starts){
  lines((i-1)*itr + (1:itr), -runs.bic[i,])
}

# after 20 iterations no model seems to converge anywhere, so I increase the number of iteration to 30. 
# It seems like BIC of model 1, 2, 3, 4 converge. Let's compare BIC of these 8 model.

min = rep(0, times = num.starts)
for (i in 1:num.starts){
  min[i] = min(runs.bic[i, ])
}

which.min(min) #4th model has lowest BIC and converge -> choose model 4


summary(lm(y~ ., data.sub[, runs[4, ] == 1]))
```

# Part III.

a) Suppose X has a standard Normal distribution and you want to estimate $\mu=E[h(X)]$ with $h(x) = \frac{x^3}{1 − e^{2x}}$. Compute a standard Monte Carlo estimator based on n = 100,000 observations.

```{r}
h = function(x){x^3/ (1 - exp(2*x))}
n = 100000

set.seed(1)
x = rnorm(n, 0, 1) 

mean(h(x)) 

#test the true value of E[h(X)]
integrand = function(x){
  h = x^3/ (1 - exp(2*x))
  f = 1/sqrt(2*pi) * exp(-x^2/2)
  return(h*f)
}
integrate(integrand, lower = -Inf, upper = Inf)

# the estimation is quite close to the true value
```

b) Consider finding $\sigma^2 = E[X^2]$ when X has the density that is proportional to $q(x) = exp(−|x|^2/4)$. Estimate $\sigma^2$ using importance sampling with standard weights.

```{r}

g = function(x){ exp(- abs(x)^2 /4)} #target function
# with standard weights I guess you mean candidate f(x) = 1
plot(function(x)g(x), -1, 1, main = "")
abline(h = 1, lty=2, lwd=2)
legend("topright", legend=c("g","f"), lty = 1:2, lwd=2, inset=0.02)

x = runif(1000) #draw from unif dist 
fg = g(x) #ratio g(xi)/f(xi)
sigma_sq = mean(fg^2)
sigma_sq
```

